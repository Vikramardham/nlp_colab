{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"formats":"ipynb,py"},"kernelspec":{"display_name":"Python 3.6.9 64-bit","language":"python","name":"python_defaultSpec_1593707895903"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"NLP_arch.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kK6CYzs5RXyY","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":627},"executionInfo":{"status":"ok","timestamp":1595888570086,"user_tz":240,"elapsed":6858,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"b93b98ae-6996-4a98-9d9c-96406744aa85"},"source":["#!pip install fasttext\n","#import fasttext as ft\n","!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 18.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 30.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 41.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d4902379ad598e76d86b0778953efbb3ab0e7e0bbbd3c6fa3b7674f95346e5d3\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O6Yvw20_xjQz","colab_type":"text"},"source":["## Text Classification Using Various NLP Architectures\n","\n","+ This notebook shows how to solve a multi-class text classification with some of the most popular architectures in NLP\n","\n","+ Including text-processing using **spaCy** as well as **Keras tokenizers**\n","\n","+ The architectures used are, **Vanilla-RNN and GRU**, implemented **from scracth**, PyTorch version of **LSTM** and the fancy **Transformers model** from Huggingface library\n","\n","+ The notebook focuses on implementations and approach to solve this NLP task and hyperparameter tuning won't be addressed here, although rather straight forward (in this case)\n","\n","+ Following this work should allow one to,\n","  + Preprocess text data using Keras or spaCy or Transformers tokenizers and convert them into word embeddings\n","  + Use PyTorch to build \n","      + custom models\n","      + setup an NLP text-classification problem, \n","      + train and \n","      + validate\n","  + Introduce to the huggingface library\n"]},{"cell_type":"markdown","metadata":{"id":"xsQdInUH4x2g","colab_type":"text"},"source":["## Mount Data on Google Drive (If running on google colab)"]},{"cell_type":"code","metadata":{"id":"7qiORrFKLmxB","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1595888533065,"user_tz":240,"elapsed":45757,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"6cfac5e4-d0fb-40ce-cf94-11ab650f12bf"},"source":["from google.colab import drive \n","drive.mount('/content/drive/')\n","data = '/content/drive/My\\ Drive/nlp/nlp_colab/'\n","% cd {data}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n","/content/drive/My Drive/nlp/nlp_colab\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-flmSh5tZUW5","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1595888746264,"user_tz":240,"elapsed":432,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"cffab762-46f5-4d99-b380-5ff04c6caea5"},"source":["%load_ext tensorboard\n","%load_ext autoreload\n","%autoreload 2\n","import io, random, os\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","\n","# Input data and train_test_split\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics \n","\n","\n","# Custom models \n","from nlp_models import *\n","from nlp_train import *\n","\n","# Tensorboard for PyTorch\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Preprocessing imports\n","import spacy\n","from keras.preprocessing import text, sequence\n","\n","## Transfomers related imports\n","from transformers import pipeline\n","from transformers import AdamW\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer\n","from transformers import Trainer, TrainingArguments\n","\n","#import fasttext as ft"],"execution_count":13,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ioikP01O48jO","colab_type":"text"},"source":["## Set random seeds for reproducibility"]},{"cell_type":"code","metadata":{"id":"NLIYX4YhlbhH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595888582995,"user_tz":240,"elapsed":1565,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["def seed_init(seed=31415):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","seed_init()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nlNdG1Hf5EIo","colab_type":"text"},"source":["## Fetch the 20 newsgroups data using the scikit-learn API"]},{"cell_type":"code","metadata":{"id":"c6zdhpqMZUXB","colab_type":"code","tags":[],"colab":{},"executionInfo":{"status":"ok","timestamp":1595894438243,"user_tz":240,"elapsed":2224,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["categories = ['sci.crypt', 'sci.electronics',\n","              'sci.med', 'sci.space', 'soc.religion.christian']\n","newsgroups_all= fetch_20newsgroups(subset='all',\n","                                      remove=('headers', 'footers', 'quotes'),\n","                                      categories=categories)\n","X_train, X_test, y_train, y_test = train_test_split(newsgroups_all.data, newsgroups_all.target,\n","                                         test_size=0.2, stratify=newsgroups_all.target)\n","X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test,\n","                                         test_size=0.5, stratify=y_test)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWVHMqGL5L1f","colab_type":"text"},"source":["## Set Basic Parameters"]},{"cell_type":"code","metadata":{"id":"pccE7MvkzHSw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595894381108,"user_tz":240,"elapsed":502,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["Nwords = 128 # Number of tokens per input\n","data_path=Path('../') # Data folder\n","seq_ln = Nwords \n","emb_sz = 300 # Size of embedding vector\n","output_sz = 5 # Size of output = # of classes\n","hd_sz = 300 # Size of the hidden units"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_LAue3OZUXE","colab_type":"text"},"source":["> # Tokenize and Create Embedding"]},{"cell_type":"code","metadata":{"id":"LrCAa-lJDhrt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595894442848,"user_tz":240,"elapsed":2048,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["## Keras preprocessing\n","# Tokenize and creat train, valid and test datasets\n","\n","tokenizer = text.Tokenizer(num_words=60000)\n","tokenizer.fit_on_texts(list(X_train)+list(X_valid) + list(X_test))\n","all_words = list(tokenizer.index_word.values())\n","\n","xtrain = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train), padding='post', maxlen=200)\n","xvalid = sequence.pad_sequences(tokenizer.texts_to_sequences(X_valid), padding='post', maxlen=200)\n","xtest = sequence.pad_sequences(tokenizer.texts_to_sequences(X_test), padding='post', maxlen=200)\n","\n","# For RNNs\n","train_data = mydataset(xtrain, y_train)\n","test = mydataset(xtest, y_test)\n","valid = mydataset(xvalid, y_valid)\n","\n","# For transformer models\n","\n","## Free Memory\n","#del X_valid\n","#del X_test\n","#del X_train\n","#del y_train\n","#del xtrain"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bmggFS9ZUXF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595888638431,"user_tz":240,"elapsed":376,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["# Spacy processing \n","# If you want to use Spacy Tokenizer's instead of Keras\n","\n","#tokenizer = Tokenizer()\n","#spc = SpacyTokenizer('en')\n","#wiki_words = pickle.load(open(data_path / 'itos_wt103.pkl','rb'))\n","#wiki_vocab = Vocab.create([wiki_words], max_vocab=60000, min_freq=1)\n","\n","#def pipeline(xin, yin):\n","#    return mydataset([pad_zeros(wiki_vocab.numericalize(i)) for i in tokenizer.process_all(xin)], yin)\n","\n","#token_train = tokenizer.process_all(X_train)\n","\n","#def pad_zeros(inp, max_len=Nwords):\n","#    ''' pad zeros if the len(input) < max_len'''\n","#    if len(inp)>=max_len:\n","#        return inp[:max_len]\n","#    else:\n","#        return inp+[0]*(max_len-len(inp))\n","    \n","#xtrain = [pad_zeros(wiki_vocab.numericalize(i)) for i in token_train]\n","#word_to_ix = {word: i for i, word in enumerate(wiki_vocab.itos)}\n","\n","#valid =pipeline(X_valid, y_valid)\n","#test = pipeline(X_test, y_test)\n","#train_data = mydataset(xtrain, y_train)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWM2w9ITZUXO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595888640736,"user_tz":240,"elapsed":1663,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["## Create Embedding vectors and dump to save time in the future\n","\n","def create_emb(vecs, itos, em_sz=300, mult=1.):\n","    emb = nn.Embedding(len(itos), em_sz, padding_idx=1)\n","    wgts = emb.weight.data\n","    vec_dic = {w:vecs.get_word_vector(w) for w in vecs.get_words()}\n","    miss = []\n","    for i,w in enumerate(itos):\n","        try: wgts[i] = tensor(vec_dic[w])\n","        except: miss.append(w)\n","    return emb\n","\n","#en_vecs = ft.load_model(str(('../cc.en.300.bin')))\n","#vocab_sz = len(wiki_vocab.itos)\n","#emb_enc = create_emb(en_vecs, wiki_vocab.itos)\n","#emb_enc = torch.load(data_path/'en_emb.pth')\n","\n","#emb_enc = create_emb(en_vecs, all_words)\n","#torch.save(emb_enc, data_path/'en_emb_keras.pth')\n","\n","vocab_sz=len(all_words)\n","emb_enc = torch.load(data_path/'en_emb_keras.pth')"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_m3Mf-nqZUXJ","colab_type":"text"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"g4goT0n5wdA2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595888644418,"user_tz":240,"elapsed":537,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["default_config = dict({'lr' : 0.01,'epochs':8, 'bs':64, 'wd':0.001})"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPafN1KYweLJ","colab_type":"text"},"source":["## Vanilla RNN\n","----"]},{"cell_type":"code","metadata":{"id":"BkVREVzmZUXp","colab_type":"code","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1595889416658,"user_tz":240,"elapsed":663681,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"c31d5ca5-481a-42e2-f639-92101076f3db"},"source":["rnn = myArch(emb_enc, seq_ln, 64, stacks=2, CHOICE='Vanilla RNN' )\n","train = SimpleTrain(rnn, nn.CrossEntropyLoss())\n","\n","train.train(train_data, valid, default_config, log=True, metrics=['acc'], sch='cos')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["----------------------------------------------------\n","Epoch:\t Train_loss \t Valid_loss\tacc\t\n","0\t 1.7284 \t 1.5644 \t 0.2707\t0.3445\t\n","---------------------------------------------\n","1\t 1.4494 \t 1.4898 \t 0.3717\t0.4438\t\n","---------------------------------------------\n","2\t 1.3411 \t 1.3666 \t 0.4545\t0.5534\t\n","---------------------------------------------\n","3\t 1.3359 \t 1.2667 \t 0.5071\t0.6380\t\n","---------------------------------------------\n","4\t 1.1551 \t 1.2172 \t 0.5111\t0.6550\t\n","---------------------------------------------\n","5\t 1.0894 \t 1.1843 \t 0.5253\t0.6865\t\n","---------------------------------------------\n","6\t 1.1317 \t 1.1701 \t 0.5333\t0.6923\t\n","---------------------------------------------\n","7\t 1.1569 \t 1.1662 \t 0.5313\t0.6926\t\n","---------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U-JQthgTwm51","colab_type":"text"},"source":["## Custom GRU"]},{"cell_type":"code","metadata":{"id":"jcycTFFOwjjm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1595890310224,"user_tz":240,"elapsed":1448730,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"49772a03-5101-434f-f7d7-0194bd145d19"},"source":["gru = myArch(emb_enc, seq_ln, 64, stacks=2, CHOICE='GRU' )\n","\n","train = SimpleTrain(gru, nn.CrossEntropyLoss())\n","train.train(train_data, valid, default_config, log=True, metrics=['acc'], sch='cos')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["----------------------------------------------------\n","Epoch:\t Train_loss \t Valid_loss\tacc\t\n","0\t 1.2146 \t 1.2913 \t 0.5010\t0.5845\t\n","---------------------------------------------\n","1\t 1.1247 \t 0.9621 \t 0.6323\t0.7259\t\n","---------------------------------------------\n","2\t 0.7957 \t 0.8037 \t 0.6970\t0.8300\t\n","---------------------------------------------\n","3\t 0.6630 \t 0.7436 \t 0.7192\t0.8689\t\n","---------------------------------------------\n","4\t 0.8300 \t 0.7169 \t 0.7030\t0.9000\t\n","---------------------------------------------\n","5\t 0.5184 \t 0.6966 \t 0.7131\t0.9177\t\n","---------------------------------------------\n","6\t 0.4324 \t 0.6769 \t 0.7313\t0.9315\t\n","---------------------------------------------\n","7\t 0.5519 \t 0.6759 \t 0.7354\t0.9356\t\n","---------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fLhoeNEsw0zC","colab_type":"text"},"source":["## PyTorch LSTM"]},{"cell_type":"code","metadata":{"id":"uTjLzmWiw1Ou","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"ok","timestamp":1595890733111,"user_tz":240,"elapsed":1869496,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"fd2c6452-6f0c-4341-c8a7-fc8c91239816"},"source":["lstm = myArch(emb_enc, seq_ln, 64, stacks=2, CHOICE='LSTM' )\n","\n","train = SimpleTrain(lstm, nn.CrossEntropyLoss())\n","train.train(train_data, valid, default_config, log=True, metrics=['acc'], sch='cos')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["----------------------------------------------------\n","Epoch:\t Train_loss \t Valid_loss\tacc\t\n","0\t 1.3185 \t 1.3345 \t 0.4020\t0.4153\t\n","---------------------------------------------\n","1\t 0.8590 \t 1.0731 \t 0.5354\t0.5671\t\n","---------------------------------------------\n","2\t 0.4810 \t 0.9017 \t 0.6505\t0.7479\t\n","---------------------------------------------\n","3\t 0.6107 \t 0.9655 \t 0.5919\t0.7035\t\n","---------------------------------------------\n","4\t 0.3301 \t 0.8417 \t 0.6606\t0.7987\t\n","---------------------------------------------\n","5\t 0.4272 \t 0.8383 \t 0.6727\t0.8138\t\n","---------------------------------------------\n","6\t 0.2172 \t 0.8587 \t 0.6687\t0.8184\t\n","---------------------------------------------\n","7\t 0.2866 \t 0.9046 \t 0.6586\t0.8088\t\n","---------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DVEaXn9HxEM_","colab_type":"text"},"source":["## Transformer Model"]},{"cell_type":"code","metadata":{"id":"wsto0R7dxHVn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595894262314,"user_tz":240,"elapsed":743,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["## Download Tokenizer and Model\n","# Here we use the BERT model\n","MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 4\n","EPOCHS = 2\n","LEARNING_RATE = 1E-05"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"39T4FZvO6MD1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595894463282,"user_tz":240,"elapsed":837,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":["## Build Dataset compatible with Huggingface API\n","train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","model_name = 'bert-base-cased'\n","bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n","\n","training_set = CustomDataset(X_train, y_train, bert_tokenizer, 128)\n","testing_set = CustomDataset(X_test, y_test, bert_tokenizer, 128)\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"20CDLVT26U_D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595892774838,"user_tz":240,"elapsed":3267,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"7a7eac5d-4019-4e8c-e6a0-5d53337cb551"},"source":["## Implement the Bertmodel class\n","model = BertModel()\n","model.to('cuda')\n","print('Model Initiated')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Model Initiated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k7nFMhY2qZct","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1595894480270,"user_tz":240,"elapsed":486,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"04b5b6f7-6cd8-406b-bafb-050f18c8df58"},"source":["bert_tokenizer.encode_plus"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method PreTrainedTokenizerBase.encode_plus of <transformers.tokenization_bert.BertTokenizer object at 0x7fd5a4b44cc0>>"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"_AqI2B4W6fb3","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"U3F_895s6DDO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1595894661515,"user_tz":240,"elapsed":138670,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"610aa39d-9358-4875-85fa-ac941f5335bb"},"source":["model.train()\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","def train(epoch):\n","  for _, data in enumerate(training_loader, 0):\n","    ids = data['ids'].to('cuda', dtype=torch.long)\n","    mask = data['mask'].to('cuda', dtype=torch.long)\n","    token_type_ids = data['token_type_ids'].to('cuda', dtype=torch.long)\n","    targets = data['targets'].to('cuda', dtype=torch.long)\n","\n","    outputs = model(ids, mask, token_type_ids)\n","\n","    optimizer.zero_grad()\n","    loss = nn.CrossEntropyLoss()(outputs, targets)\n","    if _%100==0:\n","      print(f'Epoch: {epoch}, Loss: {loss.item()}')\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","for epoch in range(2):\n","  train(epoch)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Epoch: 0, Loss: 1.6964058876037598\n","Epoch: 0, Loss: 1.0832419395446777\n","Epoch: 0, Loss: 0.6188214421272278\n","Epoch: 0, Loss: 0.467561811208725\n","Epoch: 0, Loss: 0.5233833193778992\n","Epoch: 1, Loss: 0.26846471428871155\n","Epoch: 1, Loss: 0.759685218334198\n","Epoch: 1, Loss: 0.06976211071014404\n","Epoch: 1, Loss: 0.2792321741580963\n","Epoch: 1, Loss: 0.07832983136177063\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Aacuqku56ii7","colab_type":"text"},"source":["## Validate the Model"]},{"cell_type":"code","metadata":{"id":"_0nW2hQ_6ken","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1595894664702,"user_tz":240,"elapsed":137715,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}},"outputId":"e43b1dfe-a17e-4b61-fc86-aa412dc7a3e0"},"source":["device='cuda'\n","def validation(epoch):\n","    model.eval()\n","    fin_targets=[]\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = torch.tensor(data['targets']).to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","    return fin_outputs, fin_targets\n","\n","for epoch in range(1):\n","    outputs, targets = validation(epoch)\n","    print(len(outputs), len(targets))\n","    \n","    arg_outs = np.argmax(outputs, axis=1)\n","    accuracy = metrics.accuracy_score(targets, arg_outs)\n","    \n","    f1_score_micro = metrics.f1_score(targets, arg_outs, average='micro')\n","    f1_score_macro = metrics.f1_score(targets, arg_outs, average='macro')\n","    \n","    print(f\"Accuracy Score = {accuracy}\")\n","    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n"],"execution_count":47,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["495 495\n","Accuracy Score = 0.8747474747474747\n","F1 Score (Micro) = 0.8747474747474747\n","F1 Score (Macro) = 0.8756885392819788\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7bX49qU6q5d","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1595800432794,"user_tz":240,"elapsed":2190,"user":{"displayName":"Vikram Reddy","photoUrl":"","userId":"14647491600015104525"}}},"source":[""],"execution_count":null,"outputs":[]}]}